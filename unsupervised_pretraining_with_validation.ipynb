{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f43f782",
   "metadata": {},
   "source": [
    "# Unsupervised Pre-training with Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9a1d4",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the unsupervised pre-training process for a BERT model using **Masked Language Modeling (MLM)**. The steps include loading data, tokenization, masking, model training, and saving the pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d71a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c41514",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a91b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d958481a1a46d28b588a83f136187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load dataset (e.g., WikiText dataset)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa34c74",
   "metadata": {},
   "source": [
    "## 2. Masking for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b391a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masked inputs for MLM\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # Exclude from loss computation\n",
    "\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "\n",
    "# Prepare input data\n",
    "inputs = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "inputs, labels = mask_tokens(inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224d4bc",
   "metadata": {},
   "source": [
    "## 3. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b76a564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8fd3b6334e42e2b6f76bde40058bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model for Masked Language Modeling\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f623db",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6529cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(inputs, batch_size=32, shuffle=True)\n",
    "loop = tqdm(train_dataloader, leave=True)\n",
    "for batch in loop:\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a930f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myvenv/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/56293 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[0;32m---> 12\u001b[0m     input_ids, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# DataLoader setup\n",
    "train_dataloader = DataLoader(inputs, batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to('mps')\n",
    "        labels = labels.to('mps')\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e1bae",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"./pretrained_bert\")\n",
    "tokenizer.save_pretrained(\"./pretrained_bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08104f",
   "metadata": {},
   "source": [
    "## 6. Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0551e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare validation data (reusing tokenized dataset)\n",
    "validation_inputs = torch.tensor(tokenized_dataset[\"input_ids\"][:1000])  # Using a subset for validation\n",
    "validation_inputs, validation_labels = mask_tokens(validation_inputs, tokenizer)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, validation_inputs, validation_labels, batch_size=32):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(list(zip(validation_inputs, validation_labels)), batch_size=batch_size)\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(\"mps\")\n",
    "            labels = labels.to(\"mps\")\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            for pred, label in zip(pred_labels, labels):\n",
    "                predictions.extend(pred[label != -100].tolist())\n",
    "                true_labels.extend(label[label != -100].tolist())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate validation accuracy\n",
    "validation_accuracy = validate_model(model, validation_inputs, validation_labels)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2b1a4",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Loss and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0050749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated training loss and validation accuracy for demonstration\n",
    "epochs = [1, 2, 3]\n",
    "training_loss = [1.2, 0.9, 0.7]\n",
    "validation_accuracies = [0.65, 0.72, 0.78]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_loss, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, validation_accuracies, marker='o', label=\"Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
