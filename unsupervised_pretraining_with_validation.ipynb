{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f43f782",
   "metadata": {},
   "source": [
    "# Unsupervised Pre-training with Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9a1d4",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the unsupervised pre-training process for a BERT model using **Masked Language Modeling (MLM)**. The steps include loading data, tokenization, masking, model training, and saving the pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d71a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c41514",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a91b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d958481a1a46d28b588a83f136187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load dataset (e.g., WikiText dataset)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa34c74",
   "metadata": {},
   "source": [
    "## 2. Masking for MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b391a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create masked inputs for MLM\n",
    "def mask_tokens(inputs, tokenizer, mlm_probability=0.15):\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # Exclude from loss computation\n",
    "\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "\n",
    "# Prepare input data\n",
    "inputs = torch.tensor(tokenized_dataset[\"input_ids\"])\n",
    "inputs, labels = mask_tokens(inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224d4bc",
   "metadata": {},
   "source": [
    "## 3. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model for Masked Language Modeling\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f623db",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a930f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader setup\n",
    "train_dataloader = DataLoader(inputs, batch_size=32, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # 3 epochs\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to('mps')\n",
    "        labels = labels.to('mps')\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e1bae",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"./pretrained_bert\")\n",
    "tokenizer.save_pretrained(\"./pretrained_bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08104f",
   "metadata": {},
   "source": [
    "## 6. Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0551e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare validation data (reusing tokenized dataset)\n",
    "validation_inputs = torch.tensor(tokenized_dataset[\"input_ids\"][:1000])  # Using a subset for validation\n",
    "validation_inputs, validation_labels = mask_tokens(validation_inputs, tokenizer)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, validation_inputs, validation_labels, batch_size=32):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(list(zip(validation_inputs, validation_labels)), batch_size=batch_size)\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(\"mps\")\n",
    "            labels = labels.to(\"mps\")\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Collect predictions and true labels\n",
    "            for pred, label in zip(pred_labels, labels):\n",
    "                predictions.extend(pred[label != -100].tolist())\n",
    "                true_labels.extend(label[label != -100].tolist())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate validation accuracy\n",
    "validation_accuracy = validate_model(model, validation_inputs, validation_labels)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2b1a4",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Loss and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0050749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated training loss and validation accuracy for demonstration\n",
    "epochs = [1, 2, 3]\n",
    "training_loss = [1.2, 0.9, 0.7]\n",
    "validation_accuracies = [0.65, 0.72, 0.78]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, training_loss, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, validation_accuracies, marker='o', label=\"Validation Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
